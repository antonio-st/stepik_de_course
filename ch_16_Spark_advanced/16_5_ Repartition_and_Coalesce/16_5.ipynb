{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d790a501-645d-4e02-ab10-79e468dd01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession,DataFrame\n",
    "from pyspark.sql.functions import col, round, to_date, year, month, sum, avg\n",
    "\n",
    "conf = (\n",
    "     SparkConf()\n",
    "    .setAppName('Simple_Spark')\n",
    "    .setMaster('local[*]')\n",
    ")\n",
    "\n",
    "\n",
    "# Spark сессия\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf)\n",
    "    # .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "254870c3-ebd4-4642-bc88-984da54439d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример данных\n",
    "data = [(i, f\"Name_{i % 5}\") for i in range(100)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a058aa00-4ec9-4d21-b1b6-cb362a636b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  0|Name_0|\n",
      "|  1|Name_1|\n",
      "|  2|Name_2|\n",
      "|  3|Name_3|\n",
      "|  4|Name_4|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa4d7b28-9b88-47bb-94de-6c9ddf9f2d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверяем начальное количество разделов\n",
    "initial_partitions = df.rdd.getNumPartitions()\n",
    "initial_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60aecf3a-0909-464e-a527-e1bfe6125d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Применяем repartition для увеличения количества разделов до 15\n",
    "df_repartitioned = df.repartition(15)\n",
    "\n",
    "# проверим партиции\n",
    "df_repartitioned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da676854-bdc7-4e8c-a1f5-4a9330976950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования для улучшения выполнения join\n",
    "other_data = [(i, f\"OtherName_{i % 5}\") for i in range(100)]\n",
    "other_df = spark.createDataFrame(other_data, [\"id\", \"other_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e1ebc8-c0cd-472b-a028-a094df1cb5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repartitioned_df = df_repartitioned.repartition(10, \"id\")\n",
    "joined_df = repartitioned_df.join(other_df, \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc59b954-b315-4c3c-a115-61b5b51dc48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+\n",
      "| id|  name| other_name|\n",
      "+---+------+-----------+\n",
      "| 26|Name_1|OtherName_1|\n",
      "| 29|Name_4|OtherName_4|\n",
      "| 65|Name_0|OtherName_0|\n",
      "| 19|Name_4|OtherName_4|\n",
      "| 54|Name_4|OtherName_4|\n",
      "+---+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c004455-b02f-4ed4-8ad8-ecccd53d9ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295159c7-04cf-448b-9341-969b53a741fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57b514c8-ce45-4988-be15-a9607b06b10f",
   "metadata": {},
   "source": [
    "#### __partitionBy__\n",
    "\n",
    "__partitionBy__ - это метод, используемый при записи данных в файлы (например, в форматах Parquet, ORC, etc.) для указания того, как данные должны быть разделены на партиции. Он определяет логику разделения данных на основе значений одного или нескольких столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "438fe9c1-cd9b-4bf7-9a29-a06f57fda09e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrameReader.csv() got an unexpected keyword argument 'numPartitions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Если читаем с CSV какого - нибудь, то также можно указать, сколько партиций должно быть\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_sales \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msales.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: DataFrameReader.csv() got an unexpected keyword argument 'numPartitions'"
     ]
    }
   ],
   "source": [
    "# Если читаем с CSV какого - нибудь, то также можно указать, сколько партиций должно быть\n",
    "df_sales = spark.read.csv(\"sales.csv\", header=True, inferSchema=True, numPartitions=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e564c2fa-74eb-432d-bd40-75b624431e78",
   "metadata": {},
   "source": [
    "#### coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a6954-85f9-4be5-8d9d-13e794513867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbee5634-c717-4e8c-906b-4903659e11f8",
   "metadata": {},
   "source": [
    "__coalesce__ — это операция в Apache Spark, которая используется для уменьшения количества партиций (разделов) в DataFrame или RDD. В отличие от repartition, которая может как увеличивать, так и уменьшать количество партиций и всегда включает shuffle (перераспределение данных между узлами), coalesce оптимизирована для уменьшения количества партиций без shuffle, если это возможно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe2feb14-dbc7-4adc-a00e-f997ca7f9ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверяем начальное количество партиций\n",
    "initial_partitions = df.rdd.getNumPartitions()\n",
    "initial_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9139a27-8f93-4fe5-9b49-c93233c67247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Применяем repartition для увеличения количества партиций до 10 (для демонстрации)\n",
    "df_repartitioned = df.repartition(10)\n",
    "df_repartitioned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6079bd58-e877-4f4f-bca3-4d97db44e2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coalesced = df_repartitioned.coalesce(3)\n",
    "df_coalesced.rdd.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
