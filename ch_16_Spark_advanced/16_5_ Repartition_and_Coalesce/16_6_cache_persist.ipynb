{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66c85edd-2b89-43d9-b261-de5bc6dde895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession,DataFrame\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import col, round, to_date, year, month, sum, avg\n",
    "\n",
    "conf = (\n",
    "     SparkConf()\n",
    "    .setAppName('Simple_Spark')\n",
    "    .setMaster('local[*]')\n",
    ")\n",
    "\n",
    "\n",
    "# Spark сессия\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf)\n",
    "    # .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "33c7f6bb-d43b-400e-852f-7cd5b2949ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.driver.host', '192.168.100.11'),\n",
       " ('spark.app.id', 'local-1726511225150'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'Simple_Spark'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.submitTime', '1726507868506'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.app.startTime', '1726511225105'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '42267')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e829e-ba03-4157-a08a-2d0648d533a1",
   "metadata": {},
   "source": [
    "cache — это метод в Apache Spark, который используется для кэширования данных в памяти. Он позволяет избежать повторного выполнения вычислений при повторном доступе к данным. Кэширование может существенно повысить производительность Spark-приложений, особенно если одни и те же данные используются многократно в различных этапах вычислений, то есть чаще всего при join операциях.\n",
    "\n",
    "Основная логика cache следующая - \n",
    "\n",
    "> Данные сохраняются в оперативной памяти всех рабочих узлов (executors) кластера.\n",
    "При повторном доступе к данным Spark извлекает их из памяти, а не выполняет вычисления заново.\n",
    "\n",
    "> Метод cache помечает DataFrame или RDD для кэширования, но фактически кэширование происходит только при первом вызове действия (action), которое вызывает выполнение вычислений (например, count, collect, show).\n",
    "\n",
    "> Если данные не помещаются в память, Spark может использовать метод persist с соответствующим уровнем хранения (storage level), чтобы сохранить их на диск "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "390892ab-67d1-4960-b945-5773ade9bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем df\n",
    "data = [(i, f\"name_{i * 5}\") for i in range(1000000)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b89d8337-aa8b-4af6-b4c9-56d169d4c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем фильтр и кэшируем результат\n",
    "filtered_df = (\n",
    "df\n",
    ".filter(col(\"id\") % 2 == 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be2bf1fa-e024-41ab-bafe-65bb89382832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/16 21:28:37 WARN CacheManager: Asked to cache already cached data.\n",
      "24/09/16 21:28:37 WARN TaskSetManager: Stage 4 contains a task of very large size (5038 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# кэшируем при action\n",
    "filtered_df.cache()\n",
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2004f7a-5095-4c48-a201-4b0d3073e5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  0| name_0|\n",
      "|  2|name_10|\n",
      "|  4|name_20|\n",
      "|  6|name_30|\n",
      "|  8|name_40|\n",
      "| 10|name_50|\n",
      "| 12|name_60|\n",
      "| 14|name_70|\n",
      "| 16|name_80|\n",
      "| 18|name_90|\n",
      "+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f330e41-52b2-4402-af94-168ca4c36bf4",
   "metadata": {},
   "source": [
    "метод __cache__ использует стандартный уровень хранения MEMORY_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46c8af-144d-4776-a163-feabadcee374",
   "metadata": {},
   "source": [
    "#### persist\n",
    "\n",
    "    MEMORY_ONLY: Кэширует данные только в памяти. Если данные не помещаются, они не сохраняются на диск.\n",
    "    MEMORY_AND_DISK: Кэширует данные в памяти и на диске, если данные не помещаются в память.\n",
    "    DISK_ONLY: Кэширует данные только на диске.\n",
    "    MEMORY_ONLY_SER: Кэширует данные в памяти в сериализованном виде. Это уменьшает объем памяти, но увеличивает время доступа.\n",
    "    MEMORY_AND_DISK_SER: Кэширует данные в памяти и на диске в сериализованном виде.\n",
    "    OFF_HEAP: Кэширует данные в памяти вне кучи (off-heap), что может уменьшить накладные расходы на сборку мусора (Garbage Collection).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b01d91df-9834-4f7d-a8a4-9ea404f4c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/16 22:03:29 WARN TaskSetManager: Stage 15 contains a task of very large size (5038 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/09/16 22:03:30 WARN TaskSetManager: Stage 16 contains a task of very large size (5038 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.persist(StorageLevel.DISK_ONLY)\n",
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20927b8a-6fef-4706-920a-deaddf04961b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/16 21:22:10 WARN TaskSetManager: Stage 48 contains a task of very large size (5038 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/09/16 21:22:11 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/09/16 21:22:11 WARN BlockManager: Block rdd_160_1 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/09/16 21:22:11 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/09/16 21:22:11 WARN BlockManager: Block rdd_160_3 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/09/16 21:22:11 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/09/16 21:22:11 WARN BlockManager: Block rdd_160_0 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/09/16 21:22:11 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/09/16 21:22:11 WARN BlockManager: Block rdd_160_2 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/09/16 21:22:11 WARN TaskSetManager: Stage 49 contains a task of very large size (5038 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.persist(StorageLevel.MEMORY_AND_DISK_2)\n",
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "356ab9e0-59de-4ccc-a09b-2d00282888c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/16 21:24:59 WARN TaskSetManager: Stage 52 contains a task of very large size (5038 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/09/16 21:25:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/09/16 21:25:00 WARN BlockManager: Block rdd_171_1 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/09/16 21:25:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/09/16 21:25:00 WARN BlockManager: Block rdd_171_0 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/09/16 21:25:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/09/16 21:25:00 WARN BlockManager: Block rdd_171_2 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/09/16 21:25:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/09/16 21:25:00 WARN BlockManager: Block rdd_171_3 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/09/16 21:25:01 WARN TaskSetManager: Stage 53 contains a task of very large size (5038 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.persist(StorageLevel.DISK_ONLY_2)\n",
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e97d7380-ca73-4cd6-b246-0e5717ee597a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/16 22:03:01 WARN TaskSetManager: Stage 11 contains a task of very large size (5038 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/09/16 22:03:02 WARN TaskSetManager: Stage 12 contains a task of very large size (5038 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.cache()\n",
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1050360e-5382-4f65-9c3a-5b1760368a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.unpersist(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8979dded-912a-4a04-b339-53d2cb770616",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
