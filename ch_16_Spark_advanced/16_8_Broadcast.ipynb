{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7383c56a-a9a8-425e-8e77-fd4636387774",
   "metadata": {},
   "source": [
    "Broadcast переменные используются для эффективного распределения больших объектов или небольших наборов данных по всем узлам кластера. Это позволяет избежать многократной передачи одних и тех же данных при выполнении распределенных вычислений. Broadcast переменные полезны, когда у вас есть неизменяемые данные, которые нужно использовать в нескольких местах в вашем приложении Spark, и эти данные достаточно велики, чтобы их передача в каждый узел была неэффективной.\n",
    "\n",
    "Ничего не понятно, но очень интересно... Давайте разбираться.\n",
    "\n",
    "Я предвкушаю Ваши вопросы по поводу того, а в чем разница в сравнении с cache и с persist соответственно? В ходе объяснения постараюсь рассказать Вам эту разницу.\n",
    "\n",
    "Broadcast чаще всего используется в случае, когда у Вас есть какой-то справочник с данными. То есть он не будет уже изменяться. И, поскольку, он хранится на всех узлах одновременно, то у нас нет необходимости каждый раз вызывать этап shuffle для перемещения ключей. Приведу простой пример, когда это может использоваться. \n",
    "\n",
    "Например, если у вас есть большой DataFrame и маленький DataFrame, и вы хотите выполнить join, оптимальным решением будет использовать broadcast переменную для маленького DataFrame. Это позволяет избежать дорогих shuffle операций, так как маленький DataFrame будет распространяться по всем узлам и join будет выполняться локально на каждом узле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f25f3343-9970-4b8a-a3ac-9d6522533965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, col\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Broadcast Join Example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c27c25-eafa-46bc-b0aa-ba9a41f2d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример большого DataFrame\n",
    "large_data = [(i, f\"Name_{i % 5}\") for i in range(1000000)]\n",
    "large_df = spark.createDataFrame(large_data, [\"id\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94383ecb-3349-4b04-93c1-f038a5a03d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  0|Name_0|\n",
      "|  1|Name_1|\n",
      "|  2|Name_2|\n",
      "+---+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/17 20:59:46 WARN TaskSetManager: Stage 1 contains a task of very large size (3791 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "large_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab129e14-b540-4e59-9a65-38aff253e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id| department|\n",
      "+---+-----------+\n",
      "|  1|         HR|\n",
      "|  2|    Finance|\n",
      "|  3|Engineering|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Пример маленького DataFrame\n",
    "small_data = [(1, \"HR\"), (2, \"Finance\"), (3, \"Engineering\")]\n",
    "small_df = spark.createDataFrame(small_data, [\"id\", \"department\"])\n",
    "small_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f4c9689-4abd-4bc8-a188-099e7d269f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение broadcast к маленькому DataFrame\n",
    "broadcast_df = broadcast(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "059ffd77-dfad-4359-a9d1-f2b56d420d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_df = large_df.join(broadcast_df, large_df.id == broadcast_df.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2695afe-2607-4837-9c15-19ee2eec188a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/17 21:08:37 WARN TaskSetManager: Stage 5 contains a task of very large size (3791 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/09/17 21:08:38 WARN TaskSetManager: Stage 6 contains a task of very large size (3919 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+-----------+\n",
      "| id|  name| id| department|\n",
      "+---+------+---+-----------+\n",
      "|  1|Name_1|  1|         HR|\n",
      "|  2|Name_2|  2|    Finance|\n",
      "|  3|Name_3|  3|Engineering|\n",
      "+---+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "j_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
